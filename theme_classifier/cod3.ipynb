{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cod3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT7YDxlYU_-t",
        "outputId": "254c2e9f-4fcf-4be4-edf0-78c8c92bf737"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "!pip3 install cltk\n",
        "import cltk\n",
        "from cltk.tokenize.sentence import TokenizeSentence\n",
        "from cltk.stop.classical_hindi.stops import STOPS_LIST\n",
        "\n",
        "!pip3 install stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: cltk in /usr/local/lib/python3.6/dist-packages (0.1.121)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.6/dist-packages (from cltk) (0.9.7)\n",
            "Requirement already satisfied: pyuca in /usr/local/lib/python3.6/dist-packages (from cltk) (1.2)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (from cltk) (2.7.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from cltk) (3.13)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.6/dist-packages (from cltk) (3.1.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from cltk) (3.2.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from cltk) (2019.12.20)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython->cltk) (4.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->cltk) (1.15.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython->cltk) (3.0.4)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEAiyFudgo47",
        "outputId": "69924ebe-4e92-4522-81d5-0d067b8b2e60"
      },
      "source": [
        "import stanza\n",
        "stanza.download('hi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 30.0MB/s]                    \n",
            "2020-11-29 13:10:16 INFO: Downloading default packages for language: hi (Hindi)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/hi/default.zip: 100%|██████████| 208M/208M [00:09<00:00, 21.3MB/s]\n",
            "2020-11-29 13:10:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-z8IPybg-81"
      },
      "source": [
        "hindi_txt = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akPRXFJfVTaq"
      },
      "source": [
        "input_path = \"/content/drive/My Drive/Colab Notebooks (1)/cod/data/all_classes_data.txt\"\n",
        "input_path2 = \"/content/drive/My Drive/Colab Notebooks (1)/cod/data/all_classes_data2.txt\"\n",
        "input_path3 = \"/content/drive/MyDrive/Colab Notebooks (1)/cod/data/new_data_labels.csv\"\n",
        "stopwords_path = \"/content/drive/My Drive/Colab Notebooks (1)/cod/data/stopwords.txt\"\n",
        "\n",
        "entire_data_path = \"/content/drive/My Drive/Colab Notebooks (1)/cod/data/processed.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6AgnL_czW6l"
      },
      "source": [
        "main_tags = ['local_news','health','education','employment', 'prices_inequality', 'industry','migration', 'infrastructure_services', 'consumer_issues', 'culture_&_entertainment', 'environment','agriculture','livelihood', 'social_issues', 'governance','community_groups' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V72gNtQkhrgl",
        "outputId": "c81633f6-9c93-4aa7-c054-9dbd3d7ccfeb"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fastText' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJl_3SATEl77",
        "outputId": "c9b8829a-7f24-4e8b-ea5b-67b9cae6da84"
      },
      "source": [
        "%cd fastText\n",
        "!pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fastText\n",
            "Processing /content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3030398 sha256=a56c7a75547de042ae1baf7ea93e1a7de84e78d72a35598227da52f8c48bc304\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lrj6rtzx/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "  Found existing installation: fasttext 0.9.2\n",
            "    Uninstalling fasttext-0.9.2:\n",
            "      Successfully uninstalled fasttext-0.9.2\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXsiaLIacjQ6"
      },
      "source": [
        "import fasttext.util\n",
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrd3iDJ5doa6",
        "outputId": "d484ad0d-778b-471c-e040-d0cf55443ff6"
      },
      "source": [
        "# load embeddings into memory\n",
        "path_pre_trained = \"/content/drive/My Drive/Colab Notebooks (1)/cod/cc.hi.300.bin\"\n",
        "print(\"Loading embeddings\")\n",
        "ft = fasttext.load_model(path_pre_trained)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1OpSGcaJlG"
      },
      "source": [
        "# fasttext.util.reduce_model(ft, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEQiyqLyCcJZ",
        "outputId": "c8a3dace-515c-4da6-e808-bad96f9dca8a"
      },
      "source": [
        "ft.get_nearest_neighbors(b'\\xe0\\xa4\\x95\\xe0\\xa4\\xb0\\xe0\\xa4\\xa8\\xe0\\xa5\\x87')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.7406541705131531, 'लिए'),\n",
              " (0.7016867995262146, 'कराने'),\n",
              " (0.6950420141220093, 'करना'),\n",
              " (0.6947993040084839, 'होने'),\n",
              " (0.6805431246757507, 'करके'),\n",
              " (0.6702057123184204, 'करवाने'),\n",
              " (0.6648045182228088, 'देने'),\n",
              " (0.6638022065162659, 'करनेे'),\n",
              " (0.6585147976875305, 'करते'),\n",
              " (0.6459359526634216, 'करता')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHv2qu4R5oTA"
      },
      "source": [
        "path_classes = \"/content/drive/MyDrive/Colab Notebooks (1)/cod/data/classes2/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz3pRlbyLSZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TnoRsyOmWYt"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teDVlVi7ZfHz"
      },
      "source": [
        "def sentence_to_vec_tfidf(s, stop_words, tokenizer, final_tf_idf, tfidf_feat, row):\n",
        "\n",
        "    # tokenize the sentence\n",
        "    # print(s)\n",
        "    words = tokenizer(s)\n",
        "    \n",
        "    # remove stop word tokens\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "        \n",
        "    sent_vec = np.zeros(300) # as word vectors are of zero length\n",
        "    print(row)\n",
        "    for w in words:\n",
        "        emb = ft.get_word_vector(w.encode('utf8'))\n",
        "        tfidf = final_tf_idf [row, tfidf_feat.index(w)]\n",
        "        sent_vec += (vec * tfidf)\n",
        "        weight_sum += tfidf\n",
        "            \n",
        "    \n",
        "    return sent_vec / np.sqrt((sent_vec ** 2).sum())\n",
        "\n",
        "# create sentence embeddings\n",
        "def create_embeddings2(sentences):\n",
        "    print(\"Creating sentence vectors\")\n",
        "\n",
        "    tf_idf_vect = TfidfVectorizer(stop_words=None, tokenizer=word_tokenize)\n",
        "    final_tf_idf = tf_idf_vect.fit_transform(sentences)\n",
        "    tfidf_feat = tf_idf_vect.get_feature_names()\n",
        "\n",
        "    vectors = []\n",
        "    row = 0\n",
        "    for review in sentences:\n",
        "        words = word_tokenize(review)\n",
        "    \n",
        "        # remove stop word tokens\n",
        "        words = [w for w in words if not w in STOPS_LIST]\n",
        "            \n",
        "        sent_vec = np.zeros(300) # as word vectors are of zero length\n",
        "        # print(row)\n",
        "        for w in words:\n",
        "            emb = ft.get_word_vector(w.encode('utf8'))\n",
        "            tfidf = final_tf_idf [row, tfidf_feat.index(w)]\n",
        "            sent_vec += (emb * tfidf)                \n",
        "        \n",
        "        if np.sqrt((sent_vec ** 2).sum()) !=0:\n",
        "            sent_vec = sent_vec / np.sqrt((sent_vec ** 2).sum())\n",
        "        vectors.append(sent_vec)\n",
        "\n",
        "        row+=1\n",
        "    return vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_fasttext_SVM2(df, cls, col, k=5):\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(df)\n",
        "\n",
        "    X = np.array(create_embeddings2(df[col]))\n",
        "    print(X.shape)\n",
        "\n",
        "    acc = []\n",
        "    i = 0\n",
        "\n",
        "    YPred = np.array([])\n",
        "    YTest = np.array([])\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for train_index, test_index in kf.split(df):\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = df[cls][train_index], df[cls][test_index]\n",
        "\n",
        "        xtrain = X_train\n",
        "        xtest = X_test\n",
        "\n",
        "        # print(type(xtrain))\n",
        "        # print(xtrain)\n",
        "\n",
        "        # initialize logistic regression model\n",
        "        model = SVC(C=10,kernel='linear')\n",
        "\n",
        "        #scaling\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(xtrain)\n",
        "        xtrainT = scaler.transform(xtrain)\n",
        "        xtestT = scaler.transform(xtest)\n",
        "\n",
        "        model.fit(xtrainT, y_train)\n",
        "\n",
        "        # make predictions on test data\n",
        "        # threshold for predictions is 0.5\n",
        "        preds = model.predict(xtestT)\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(y_test, preds)\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        # print(type(y_test), type(preds), preds.shape)\n",
        "        # print(y_test.to_numpy().shape)\n",
        "\n",
        "        YTest = np.concatenate([YTest,y_test.to_numpy()])\n",
        "        YPred = np.concatenate([YPred,preds])\n",
        "\n",
        "        print(f\"Fold: {i}\")\n",
        "        print(f\"Accuracy = {accuracy}\")\n",
        "        i+=1\n",
        "\n",
        "        # print(xtest)\n",
        "        # print(count_vec)\n",
        "        # break\n",
        "\n",
        "    # print(type(YTest))\n",
        "    # print(YTest.shape)\n",
        "    # print(\"Accuracy for {}: {}\".format(cls, sum(acc)/len(acc)))\n",
        "    print(\"Class: {}\".format(cls))\n",
        "    print(\"Accuracy score: {}\".format(accuracy_score(YTest,YPred)))\n",
        "    print(\"Precision score: {}\".format(precision_score(YTest,YPred)))\n",
        "    print(\"Recall score: {}\".format(recall_score(YTest,YPred)))\n",
        "    print(\"F1 score: {}\".format(f1_score(YTest,YPred)))\n",
        "    print(\"Confusion Matrix: {}\".format(confusion_matrix(YTest,YPred)))\n",
        "    print(\"\")\n",
        "\n",
        "    row.append(accuracy_score(YTest,YPred))\n",
        "    row.append(confusion_matrix(YTest,YPred))\n",
        "    row.append(precision_score(YTest,YPred))\n",
        "    row.append(recall_score(YTest,YPred))\n",
        "    row.append(f1_score(YTest,YPred))\n",
        "    \n",
        "    return row\n",
        "\n",
        "\n",
        "def train_all_classes2(classes, col):\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for cls in classes:\n",
        "        path = path_classes+cls+\".csv\"\n",
        "        df_s = pd.read_csv(path,encoding='utf8')\n",
        "\n",
        "        df_s = df_s[df_s[col].notnull()]\n",
        "        df_s = df_s.reset_index(drop=True)\n",
        "\n",
        "        r = train_fasttext_SVM2(df_s,cls,col)\n",
        "        rows.append(r)\n",
        "    \n",
        "    newdf = pd.DataFrame(rows)\n",
        "    return newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue0QyE5Za9UX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "30835341-e6f3-4219-fe7c-c08932f5745f"
      },
      "source": [
        "newdf = train_all_classes2(main_tags,'only_hindi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating sentence vectors\n",
            "(9814, 300)\n",
            "Fold: 0\n",
            "Accuracy = 0.5150280183392766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-ecaf69f5c7ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all_classes2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'only_hindi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-7c02969e5434>\u001b[0m in \u001b[0;36mtrain_all_classes2\u001b[0;34m(classes, col)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mdf_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fasttext_SVM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-7c02969e5434>\u001b[0m in \u001b[0;36mtrain_fasttext_SVM2\u001b[0;34m(df, cls, col, k)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mxtestT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrainT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# make predictions on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXJwlXqbOKma"
      },
      "source": [
        "path = path_classes+\"education.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeeN_nmEOREt"
      },
      "source": [
        "df_s = pd.read_csv(path,encoding='utf8')\n",
        "\n",
        "df_s = df_s[df_s['only_hindi'].notnull()]\n",
        "df_s = df_s.reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nXr5-DM2OZRu",
        "outputId": "c6bb7b31-2791-4fb2-fe77-a2b3cf7fe60c"
      },
      "source": [
        "df_s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>level_0</th>\n",
              "      <th>index</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>agriculture</th>\n",
              "      <th>community_groups</th>\n",
              "      <th>consumer_issues</th>\n",
              "      <th>culture_&amp;_entertainment</th>\n",
              "      <th>education</th>\n",
              "      <th>employment</th>\n",
              "      <th>environment</th>\n",
              "      <th>governance</th>\n",
              "      <th>health</th>\n",
              "      <th>industry</th>\n",
              "      <th>infrastructure_services</th>\n",
              "      <th>livelihood</th>\n",
              "      <th>local_news</th>\n",
              "      <th>migration</th>\n",
              "      <th>nan</th>\n",
              "      <th>prices_inequality</th>\n",
              "      <th>social_issues</th>\n",
              "      <th>processed_sen</th>\n",
              "      <th>removed_loc</th>\n",
              "      <th>remove_loc_name</th>\n",
              "      <th>removed_loc_name</th>\n",
              "      <th>stop_word_removal</th>\n",
              "      <th>only_hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1 किलोमीटर आगे मत सेंड कर रही हूं यहां पर इंजी...</td>\n",
              "      <td>infrastructure_services, education,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1 किलोमीटर आगे मत सेंड कर रही हूं यहां पर इंजी...</td>\n",
              "      <td>1 किलोमीटर आगे मत सेंड कर रही हूं यहां पर इंजी...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 किलोमीटर आगे मत सेंड कर रही हूं यहां पर इंजी...</td>\n",
              "      <td>1 किलोमीटर आगे मत सेंड कर रही हूं यहां इंजीनिय...</td>\n",
              "      <td>किलोमीटर आगे सेंड रही हूं यहां इंजीनियरिंग कॉल...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>1 तारीख को 8:00 बजे स्कूलों में बच्चों को स्कू...</td>\n",
              "      <td>education, health, governance, governance,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1 तारीख को 8:00 बजे स्कूलों में बच्चों को स्कू...</td>\n",
              "      <td>1 तारीख को 8:00 बजे स्कूलों में बच्चों को स्कू...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 तारीख को 8:00 बजे स्कूलों में बच्चों को स्कू...</td>\n",
              "      <td>1 तारीख 8:00 बजे स्कूलों बच्चों स्कूल रखो</td>\n",
              "      <td>तारीख बजे स्कूलों बच्चों स्कूल रखो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>1 दिन में 24000 छात्रों की जानकारी वीडियो ने क...</td>\n",
              "      <td>education</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1 दिन में 24000 छात्रों की जानकारी वीडियो ने क...</td>\n",
              "      <td>1 दिन में 24000 छात्रों की जानकारी वीडियो ने क...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 दिन में 24000 छात्रों की जानकारी वीडियो ने क...</td>\n",
              "      <td>1 दिन 24000 छात्रों जानकारी वीडियो कहा समग्र आ...</td>\n",
              "      <td>दिन छात्रों जानकारी वीडियो कहा समग्र आईडी पोर्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>10 फरवरी 9 सितंबर को रिजर्वेशन सैटरडे सुपरकॉप ...</td>\n",
              "      <td>health, governance, education,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10 फरवरी 9 सितंबर को रिजर्वेशन सैटरडे सुपरकॉप ...</td>\n",
              "      <td>10 फरवरी 9 सितंबर को रिजर्वेशन सैटरडे सुपरकॉप ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10 फरवरी 9 सितंबर को रिजर्वेशन सैटरडे सुपरकॉप ...</td>\n",
              "      <td>10 फरवरी 9 सितंबर रिजर्वेशन सैटरडे सुपरकॉप सुप...</td>\n",
              "      <td>फरवरी सितंबर रिजर्वेशन सैटरडे सुपरकॉप सुपर विल...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>10 साल का कोई अच्छा मोहम्मद लोगों के सपनों को ...</td>\n",
              "      <td>health, health, education</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10 साल का कोई अच्छा मोहम्मद लोगों के सपनों को ...</td>\n",
              "      <td>10 साल का कोई अच्छा मोहम्मद लोगों के सपनों को ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10 साल का कोई अच्छा लोगों के सपनों को पूरा करत...</td>\n",
              "      <td>10 साल कोई अच्छा लोगों सपनों पूरा करती उसका बच...</td>\n",
              "      <td>साल कोई अच्छा लोगों सपनों पूरा करती उसका बच्चा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6619</th>\n",
              "      <td>6619</td>\n",
              "      <td>14480</td>\n",
              "      <td>14615</td>\n",
              "      <td>14615</td>\n",
              "      <td>14616</td>\n",
              "      <td>मेरा नाम आनंद कुमार सिंह है मैं बसिया ग्राम बो...</td>\n",
              "      <td>local_news, infrastructure_services,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मेरा नाम आनंद कुमार सिंह है मैं बसिया ग्राम बो...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मेरा नाम सिंह है मैं बसिया ग्राम बोल रहा हूं प...</td>\n",
              "      <td>मेरा नाम सिंह मैं बसिया ग्राम बोल रहा हूं पंचा...</td>\n",
              "      <td>मेरा नाम सिंह मैं बसिया ग्राम बोल रहा हूं पंचा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6620</th>\n",
              "      <td>6620</td>\n",
              "      <td>13683</td>\n",
              "      <td>13813</td>\n",
              "      <td>13813</td>\n",
              "      <td>13814</td>\n",
              "      <td>मां पिंटू कुमार राम युवा वाहिनी झारखंड हजारीबा...</td>\n",
              "      <td>governance, governance</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मां पिंटू कुमार राम युवा वाहिनी झारखंड से महिल...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मां राम युवा वाहिनी झारखंड से महिला जनता जल पर...</td>\n",
              "      <td>मां राम युवा वाहिनी झारखंड महिला जनता जल परीक्...</td>\n",
              "      <td>मां राम युवा वाहिनी झारखंड महिला जनता परीक्षण ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6621</th>\n",
              "      <td>6621</td>\n",
              "      <td>4086</td>\n",
              "      <td>4125</td>\n",
              "      <td>4125</td>\n",
              "      <td>4126</td>\n",
              "      <td>तरुण शर्मा तरुण कुमार मोबाइल पानी चमोली जिले क...</td>\n",
              "      <td>local_news,</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>तरुण शर्मा तरुण कुमार मोबाइल पानी जिले के मुख्...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मोबाइल पानी जिले के मुख्य मार्ग पर जमुना मोड़ ...</td>\n",
              "      <td>मोबाइल पानी जिले मुख्य मार्ग जमुना मोड़ समीप स...</td>\n",
              "      <td>मोबाइल पानी जिले मुख्य मार्ग जमुना मोड़ समीप स...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6622</th>\n",
              "      <td>6622</td>\n",
              "      <td>4338</td>\n",
              "      <td>4383</td>\n",
              "      <td>4383</td>\n",
              "      <td>4384</td>\n",
              "      <td>दूसरों से बोल रहा हूं स्टेशन में औरतों का जमीन...</td>\n",
              "      <td>industry, governance, governance</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>दूसरों से बोल रहा हूं स्टेशन में औरतों का जमीन...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>दूसरों से बोल रहा हूं स्टेशन में औरतों का जमीन...</td>\n",
              "      <td>दूसरों बोल रहा हूं स्टेशन औरतों जमीन कब्जा करक...</td>\n",
              "      <td>दूसरों बोल रहा हूं स्टेशन औरतों जमीन कब्जा करक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6623</th>\n",
              "      <td>6623</td>\n",
              "      <td>16625</td>\n",
              "      <td>16776</td>\n",
              "      <td>16776</td>\n",
              "      <td>16777</td>\n",
              "      <td>मैं निरंजन सिंह चौहान सतना मध्य प्रदेश के प्रो...</td>\n",
              "      <td>governance, governance</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मैं निरंजन सिंह चौहान मध्य प्रदेश के प्रोग्राम...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>मैं मध्य प्रदेश के प्रोग्राम जैसा कि आज मुझे म...</td>\n",
              "      <td>मैं मध्य प्रदेश प्रोग्राम जैसा आज मुझे मोबाइल ...</td>\n",
              "      <td>मैं मध्य प्रदेश प्रोग्राम जैसा मुझे मोबाइल वाण...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6624 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                         only_hindi\n",
              "0              0  ...  किलोमीटर आगे सेंड रही हूं यहां इंजीनियरिंग कॉल...\n",
              "1              1  ...                 तारीख बजे स्कूलों बच्चों स्कूल रखो\n",
              "2              2  ...  दिन छात्रों जानकारी वीडियो कहा समग्र आईडी पोर्...\n",
              "3              3  ...  फरवरी सितंबर रिजर्वेशन सैटरडे सुपरकॉप सुपर विल...\n",
              "4              4  ...  साल कोई अच्छा लोगों सपनों पूरा करती उसका बच्चा...\n",
              "...          ...  ...                                                ...\n",
              "6619        6619  ...  मेरा नाम सिंह मैं बसिया ग्राम बोल रहा हूं पंचा...\n",
              "6620        6620  ...  मां राम युवा वाहिनी झारखंड महिला जनता परीक्षण ...\n",
              "6621        6621  ...  मोबाइल पानी जिले मुख्य मार्ग जमुना मोड़ समीप स...\n",
              "6622        6622  ...  दूसरों बोल रहा हूं स्टेशन औरतों जमीन कब्जा करक...\n",
              "6623        6623  ...  मैं मध्य प्रदेश प्रोग्राम जैसा मुझे मोबाइल वाण...\n",
              "\n",
              "[6624 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRnXxKztOuUE"
      },
      "source": [
        "sentences = df_s['only_hindi']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S12KVmn2Or23"
      },
      "source": [
        "tf_idf_vect = TfidfVectorizer(stop_words=None, tokenizer=word_tokenize)\n",
        "final_tf_idf = tf_idf_vect.fit_transform(sentences)\n",
        "tfidf_feat = tf_idf_vect.get_feature_names()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6zMn0VHOztL"
      },
      "source": [
        "\n",
        "vectors = []\n",
        "row = 0\n",
        "for review in sentences:\n",
        "    words = word_tokenize(review)\n",
        "\n",
        "    # remove stop word tokens\n",
        "    words = [w for w in words if not w in STOPS_LIST]\n",
        "        \n",
        "    sent_vec = np.zeros(300) # as word vectors are of zero length\n",
        "    # print(row)\n",
        "    for w in words:\n",
        "        emb = ft.get_word_vector(w.encode('utf8'))\n",
        "        tfidf = final_tf_idf [row, tfidf_feat.index(w)]\n",
        "        sent_vec += (emb * tfidf)                \n",
        "    \n",
        "    sent_vec = sent_vec / np.sqrt((sent_vec ** 2).sum())\n",
        "    vectors.append(sent_vec)\n",
        "\n",
        "    row+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIKLLe3PROS"
      },
      "source": [
        "vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2g5x-SXbc7c"
      },
      "source": [
        "# newdf = train_all_classes2(main_tags,'stop_word_removal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBn0GwVUs8UU"
      },
      "source": [
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer, max_words):\n",
        "    \"\"\"\n",
        "    Given a sentence and other information,\n",
        "    this function returns embedding for the whole sentence\n",
        "    :param s: sentence, string\n",
        "    :param embedding_dict: dictionary word:vector\n",
        "    :param stop_words: list of stop words, if any\n",
        "    :param tokenizer: a tokenization function\n",
        "    \"\"\"\n",
        "    # # convert sentence to string and lowercase it\n",
        "    # words = str(s).lower()\n",
        "    \n",
        "    # tokenize the sentence\n",
        "    # print(s)\n",
        "    words = tokenizer(s)\n",
        "    \n",
        "    # remove stop word tokens\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    # keep only alpha-numeric tokens\n",
        "    # words = [w for w in words if w.isalpha()]\n",
        "    \n",
        "    # initialize empty list to store embeddings\n",
        "    M = []\n",
        "    i = 0\n",
        "    for w in words:\n",
        "    # for evert word, fetch the embedding from\n",
        "    # the dictionary and append to list of\n",
        "    # embeddings\n",
        "        emb = ft.get_word_vector(w.encode('utf8'))\n",
        "        M.append(emb)\n",
        "        i+=1\n",
        "    \n",
        "    while i<max_words:\n",
        "        i+=1\n",
        "        M.append([0]*300)\n",
        "\n",
        "    # if we dont have any vectors, return zeros\n",
        "    if len(M) == 0:\n",
        "        return np.zeros(max_words)\n",
        "    \n",
        "    # convert list of embeddings to array\n",
        "    M = np.array(M)\n",
        "    M = np.ravel(M)\n",
        "        \n",
        "    # return normalized vector\n",
        "    return M / np.sqrt((M ** 2).sum())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create sentence embeddings\n",
        "def create_embeddings(sentences, mx_words):\n",
        "    print(\"Creating sentence vectors\")\n",
        "    vectors = []\n",
        "    for review in sentences:\n",
        "        vectors.append(\n",
        "            sentence_to_vec(\n",
        "                s = review,\n",
        "                embedding_dict = ft,\n",
        "                stop_words = STOPS_LIST,\n",
        "                tokenizer = word_tokenize,\n",
        "                max_words = mx_words,\n",
        "            )\n",
        "        )\n",
        "    return vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_fasttext_SVM(df, cls, col, k=3):\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(df)\n",
        "\n",
        "    mx_words = 0\n",
        "    for ind in df.index:\n",
        "        sen = df[col][ind]\n",
        "        sen = word_tokenize(sen)\n",
        "        mx_words = max(len(sen),mx_words)\n",
        "    print(\"Max_words:\", mx_words)\n",
        "\n",
        "    X = np.array(create_embeddings(df[col], mx_words))\n",
        "    print(X.shape)\n",
        "\n",
        "    acc = []\n",
        "    i = 0\n",
        "\n",
        "    YPred = np.array([])\n",
        "    YTest = np.array([])\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for train_index, test_index in kf.split(df):\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = df[cls][train_index], df[cls][test_index]\n",
        "\n",
        "        xtrain = X_train\n",
        "        xtest = X_test\n",
        "\n",
        "        # print(type(xtrain))\n",
        "        # print(xtrain)\n",
        "\n",
        "        # initialize logistic regression model\n",
        "        model = SVC(C=10,kernel='linear')\n",
        "\n",
        "        #scaling\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(xtrain)\n",
        "        xtrainT = scaler.transform(xtrain)\n",
        "        xtestT = scaler.transform(xtest)\n",
        "\n",
        "        model.fit(xtrainT, y_train)\n",
        "\n",
        "        # make predictions on test data\n",
        "        # threshold for predictions is 0.5\n",
        "        preds = model.predict(xtestT)\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(y_test, preds)\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        # print(type(y_test), type(preds), preds.shape)\n",
        "        # print(y_test.to_numpy().shape)\n",
        "\n",
        "        YTest = np.concatenate([YTest,y_test.to_numpy()])\n",
        "        YPred = np.concatenate([YPred,preds])\n",
        "\n",
        "        # print(f\"Fold: {i}\")\n",
        "        # print(f\"Accuracy = {accuracy}\")\n",
        "        i+=1\n",
        "\n",
        "        # print(xtest)\n",
        "        # print(count_vec)\n",
        "        # break\n",
        "\n",
        "    # print(type(YTest))\n",
        "    # print(YTest.shape)\n",
        "    # print(\"Accuracy for {}: {}\".format(cls, sum(acc)/len(acc)))\n",
        "    print(\"Class: {}\".format(cls))\n",
        "    print(\"Accuracy score: {}\".format(accuracy_score(YTest,YPred)))\n",
        "    print(\"Precision score: {}\".format(precision_score(YTest,YPred)))\n",
        "    print(\"Recall score: {}\".format(recall_score(YTest,YPred)))\n",
        "    print(\"F1 score: {}\".format(f1_score(YTest,YPred)))\n",
        "    print(\"Confusion Matrix: {}\".format(confusion_matrix(YTest,YPred)))\n",
        "    print(\"\")\n",
        "\n",
        "    row.append(accuracy_score(YTest,YPred))\n",
        "    row.append(confusion_matrix(YTest,YPred))\n",
        "    row.append(precision_score(YTest,YPred))\n",
        "    row.append(recall_score(YTest,YPred))\n",
        "    row.append(f1_score(YTest,YPred))\n",
        "    \n",
        "    return row\n",
        "\n",
        "\n",
        "def train_all_classes(classes, col):\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for cls in classes:\n",
        "        path = path_classes+cls+\".csv\"\n",
        "        df_s = pd.read_csv(path,encoding='utf8')\n",
        "\n",
        "        df_s = df_s[df_s[col].notnull()]\n",
        "        df_s = df_s.reset_index(drop=True)\n",
        "\n",
        "        r = train_fasttext_SVM(df_s,cls,col)\n",
        "        rows.append(r)\n",
        "    \n",
        "    newdf = pd.DataFrame(rows)\n",
        "    return newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVSXJzP8iFOZ"
      },
      "source": [
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
        "    \"\"\"\n",
        "    Given a sentence and other information,\n",
        "    this function returns embedding for the whole sentence\n",
        "    :param s: sentence, string\n",
        "    :param embedding_dict: dictionary word:vector\n",
        "    :param stop_words: list of stop words, if any\n",
        "    :param tokenizer: a tokenization function\n",
        "    \"\"\"\n",
        "    # # convert sentence to string and lowercase it\n",
        "    # words = str(s).lower()\n",
        "    \n",
        "    # tokenize the sentence\n",
        "    # print(s)\n",
        "    words = tokenizer(s)\n",
        "    \n",
        "    # remove stop word tokens\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    # keep only alpha-numeric tokens\n",
        "    # words = [w for w in words if w.isalpha()]\n",
        "    \n",
        "    # initialize empty list to store embeddings\n",
        "    M = []\n",
        "    for w in words:\n",
        "    # for evert word, fetch the embedding from\n",
        "    # the dictionary and append to list of\n",
        "    # embeddings\n",
        "        emb = ft.get_word_vector(w.encode('utf8'))\n",
        "        M.append(emb)\n",
        "            \n",
        "    # if we dont have any vectors, return zeros\n",
        "    if len(M) == 0:\n",
        "        return np.zeros(300)\n",
        "    \n",
        "    # convert list of embeddings to array\n",
        "    M = np.array(M)\n",
        "    \n",
        "    # calculate sum over axis=0\n",
        "    v = M.sum(axis=0)\n",
        "    \n",
        "    # return normalized vector\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create sentence embeddings\n",
        "def create_embeddings(sentences):\n",
        "    print(\"Creating sentence vectors\")\n",
        "    vectors = []\n",
        "    for review in sentences:\n",
        "        vectors.append(\n",
        "            sentence_to_vec(\n",
        "                s = review,\n",
        "                embedding_dict = ft,\n",
        "                stop_words = STOPS_LIST,\n",
        "                tokenizer = word_tokenize,\n",
        "            )\n",
        "        )\n",
        "    return vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_fasttext_SVM(df, cls, col, k=3):\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(df)\n",
        "\n",
        "    X = np.array(create_embeddings(df[col]))\n",
        "    print(X.shape)\n",
        "\n",
        "    acc = []\n",
        "    i = 0\n",
        "\n",
        "    YPred = np.array([])\n",
        "    YTest = np.array([])\n",
        "\n",
        "    row = []\n",
        "\n",
        "    for train_index, test_index in kf.split(df):\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = df[cls][train_index], df[cls][test_index]\n",
        "\n",
        "        xtrain = X_train\n",
        "        xtest = X_test\n",
        "\n",
        "        # print(type(xtrain))\n",
        "        # print(xtrain)\n",
        "\n",
        "        # initialize logistic regression model\n",
        "        model = SVC(C=1,kernel='linear')\n",
        "\n",
        "        #scaling\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(xtrain)\n",
        "        xtrainT = scaler.transform(xtrain)\n",
        "        xtestT = scaler.transform(xtest)\n",
        "\n",
        "        model.fit(xtrainT, y_train)\n",
        "\n",
        "        # make predictions on test data\n",
        "        # threshold for predictions is 0.5\n",
        "        preds = model.predict(xtestT)\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(y_test, preds)\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        # print(type(y_test), type(preds), preds.shape)\n",
        "        # print(y_test.to_numpy().shape)\n",
        "\n",
        "        YTest = np.concatenate([YTest,y_test.to_numpy()])\n",
        "        YPred = np.concatenate([YPred,preds])\n",
        "\n",
        "        # print(f\"Fold: {i}\")\n",
        "        # print(f\"Accuracy = {accuracy}\")\n",
        "        i+=1\n",
        "\n",
        "        # print(xtest)\n",
        "        # print(count_vec)\n",
        "        # break\n",
        "\n",
        "    # print(type(YTest))\n",
        "    # print(YTest.shape)\n",
        "    # print(\"Accuracy for {}: {}\".format(cls, sum(acc)/len(acc)))\n",
        "    print(\"Class: {}\".format(cls))\n",
        "    print(\"Accuracy score: {}\".format(accuracy_score(YTest,YPred)))\n",
        "    print(\"Precision score: {}\".format(precision_score(YTest,YPred)))\n",
        "    print(\"Recall score: {}\".format(recall_score(YTest,YPred)))\n",
        "    print(\"F1 score: {}\".format(f1_score(YTest,YPred)))\n",
        "    print(\"Confusion Matrix: {}\".format(confusion_matrix(YTest,YPred)))\n",
        "    print(\"\")\n",
        "\n",
        "    row.append(accuracy_score(YTest,YPred))\n",
        "    row.append(confusion_matrix(YTest,YPred))\n",
        "    row.append(precision_score(YTest,YPred))\n",
        "    row.append(recall_score(YTest,YPred))\n",
        "    row.append(f1_score(YTest,YPred))\n",
        "    \n",
        "    return row\n",
        "\n",
        "\n",
        "def train_all_classes(classes, col):\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for cls in classes:\n",
        "        path = path_classes+cls+\".csv\"\n",
        "        df_s = pd.read_csv(path,encoding='utf8')\n",
        "\n",
        "        df_s = df_s[df_s[col].notnull()]\n",
        "        df_s = df_s.reset_index(drop=True)\n",
        "\n",
        "        r = train_fasttext_SVM(df_s,cls,col)\n",
        "        rows.append(r)\n",
        "    \n",
        "    newdf = pd.DataFrame(rows)\n",
        "    return newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIiBNg9xtxKR",
        "outputId": "fd4d2197-574c-421a-8257-b1c1023d4e9f"
      },
      "source": [
        "newdf = train_all_classes(main_tags,'only_hindi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating sentence vectors\n",
            "(9814, 300)\n",
            "Class: local_news\n",
            "Accuracy score: 0.43152639087018546\n",
            "Precision score: 0.42869269949066213\n",
            "Recall score: 0.41165681679233745\n",
            "F1 score: 0.4200020792182139\n",
            "Confusion Matrix: [[2215 2692]\n",
            " [2887 2020]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(10296, 300)\n",
            "Class: health\n",
            "Accuracy score: 0.47387334887334887\n",
            "Precision score: 0.4728994559742091\n",
            "Recall score: 0.4559052059052059\n",
            "F1 score: 0.46424685985560277\n",
            "Confusion Matrix: [[2532 2616]\n",
            " [2801 2347]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(6624, 300)\n",
            "Class: education\n",
            "Accuracy score: 0.6431159420289855\n",
            "Precision score: 0.6509554140127388\n",
            "Recall score: 0.6171497584541062\n",
            "F1 score: 0.6336019838809671\n",
            "Confusion Matrix: [[2216 1096]\n",
            " [1268 2044]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(2196, 300)\n",
            "Class: employment\n",
            "Accuracy score: 0.6188524590163934\n",
            "Precision score: 0.6143733567046451\n",
            "Recall score: 0.6384335154826958\n",
            "F1 score: 0.6261723983921393\n",
            "Confusion Matrix: [[658 440]\n",
            " [397 701]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(2170, 300)\n",
            "Class: prices_inequality\n",
            "Accuracy score: 0.5663594470046083\n",
            "Precision score: 0.5692307692307692\n",
            "Recall score: 0.5456221198156682\n",
            "F1 score: 0.5571764705882354\n",
            "Confusion Matrix: [[637 448]\n",
            " [493 592]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(488, 300)\n",
            "Class: industry\n",
            "Accuracy score: 0.639344262295082\n",
            "Precision score: 0.6440677966101694\n",
            "Recall score: 0.6229508196721312\n",
            "F1 score: 0.6333333333333334\n",
            "Confusion Matrix: [[160  84]\n",
            " [ 92 152]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(390, 300)\n",
            "Class: migration\n",
            "Accuracy score: 0.7\n",
            "Precision score: 0.6839622641509434\n",
            "Recall score: 0.7435897435897436\n",
            "F1 score: 0.7125307125307125\n",
            "Confusion Matrix: [[128  67]\n",
            " [ 50 145]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(6284, 300)\n",
            "Class: infrastructure_services\n",
            "Accuracy score: 0.6082113303628263\n",
            "Precision score: 0.6134846461949266\n",
            "Recall score: 0.58497772119669\n",
            "F1 score: 0.5988921472792441\n",
            "Confusion Matrix: [[1984 1158]\n",
            " [1304 1838]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(578, 300)\n",
            "Class: consumer_issues\n",
            "Accuracy score: 0.6384083044982699\n",
            "Precision score: 0.630718954248366\n",
            "Recall score: 0.6678200692041523\n",
            "F1 score: 0.6487394957983194\n",
            "Confusion Matrix: [[176 113]\n",
            " [ 96 193]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(10542, 300)\n",
            "Class: culture_&_entertainment\n",
            "Accuracy score: 0.6771011193321951\n",
            "Precision score: 0.683290791282152\n",
            "Recall score: 0.6602162777461582\n",
            "F1 score: 0.6715553840216133\n",
            "Confusion Matrix: [[3658 1613]\n",
            " [1791 3480]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(1638, 300)\n",
            "Class: environment\n",
            "Accuracy score: 0.612942612942613\n",
            "Precision score: 0.6057142857142858\n",
            "Recall score: 0.6471306471306472\n",
            "F1 score: 0.6257378984651712\n",
            "Confusion Matrix: [[474 345]\n",
            " [289 530]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(3708, 300)\n",
            "Class: agriculture\n",
            "Accuracy score: 0.6995685005393744\n",
            "Precision score: 0.7057842046718577\n",
            "Recall score: 0.6844660194174758\n",
            "F1 score: 0.6949616648411829\n",
            "Confusion Matrix: [[1325  529]\n",
            " [ 585 1269]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(1350, 300)\n",
            "Class: livelihood\n",
            "Accuracy score: 0.5259259259259259\n",
            "Precision score: 0.5279106858054227\n",
            "Recall score: 0.49037037037037035\n",
            "F1 score: 0.5084485407066053\n",
            "Confusion Matrix: [[379 296]\n",
            " [344 331]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(1142, 300)\n",
            "Class: social_issues\n",
            "Accuracy score: 0.5814360770577933\n",
            "Precision score: 0.5825932504440497\n",
            "Recall score: 0.574430823117338\n",
            "F1 score: 0.5784832451499118\n",
            "Confusion Matrix: [[336 235]\n",
            " [243 328]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(19288, 300)\n",
            "Class: governance\n",
            "Accuracy score: 0.4429697221070095\n",
            "Precision score: 0.4396797543320904\n",
            "Recall score: 0.415698880132725\n",
            "F1 score: 0.4273531606438546\n",
            "Confusion Matrix: [[4535 5109]\n",
            " [5635 4009]]\n",
            "\n",
            "Creating sentence vectors\n",
            "(182, 300)\n",
            "Class: community_groups\n",
            "Accuracy score: 0.4835164835164835\n",
            "Precision score: 0.4835164835164835\n",
            "Recall score: 0.4835164835164835\n",
            "F1 score: 0.4835164835164835\n",
            "Confusion Matrix: [[44 47]\n",
            " [47 44]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCGiezGW27EO"
      },
      "source": [
        "print(newdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWXGU_FBL7H6"
      },
      "source": [
        "print(newdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTmXNpTy4Iwk"
      },
      "source": [
        "newdf['labels'] = main_tags\n",
        "newdf = newdf.set_index('labels')\n",
        "newdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTeBaOriLci9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}