{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started...\n",
      "Found 23 images belonging to 2 classes.\n",
      "Found 23 images belonging to 2 classes.\n",
      "Found 23 images belonging to 2 classes.\n",
      "Dataset generated\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 26, 26, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 43,569\n",
      "Trainable params: 43,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9086 - binary_accuracy: 0.4348WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 40 batches). You may need to use the repeat() function when building your dataset.\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9086 - binary_accuracy: 0.4348 - val_loss: 9.1826 - val_binary_accuracy: 0.5652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.18260, saving model to ./data/simple/model_AA_2.hdf5\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 20.2032 - binary_accuracy: 0.5652\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 892ms/step - loss: 15.6651 - binary_accuracy: 0.5652\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 905ms/step - loss: 5.4748 - binary_accuracy: 0.5652\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 4.2528 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 789ms/step - loss: 7.9142 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 794ms/step - loss: 8.5593 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 705ms/step - loss: 7.1241 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 4.5731 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 2.0529 - binary_accuracy: 0.4348\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Training completed in time:  0:00:10.843185\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.7450 - binary_accuracy: 0.5652\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as npimage_path_colorbar\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "# prepare an iterators for each dataset\n",
    "\n",
    "print(\"Started...\")\n",
    "\n",
    "train_it = datagen.flow_from_directory('./data/train/',target_size=(224, 224), class_mode='binary',batch_size=40)\n",
    "val_it = datagen.flow_from_directory('./data/train/',target_size=(224, 224), class_mode='binary',batch_size=40)\n",
    "test_it = datagen.flow_from_directory('./data/train/',target_size=(224, 224), class_mode='binary',batch_size=40,shuffle=False)\n",
    "# ptest_it = datagen.flow_from_directory('./data/simple/p_test/',target_size=(224, 224), class_mode='binary',batch_size=40)\n",
    "\n",
    "# I think p_test is validation \n",
    "\n",
    "print(\"Dataset generated\")\n",
    "\n",
    "num_rows = 224\n",
    "num_columns = 224\n",
    "num_channels = 3\n",
    "num_labels = 2\n",
    "# Construct model\n",
    "model_AA_2 = Sequential() # AA - After Augmentation 2- Validation increased\n",
    "model_AA_2.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model_AA_2.add(MaxPooling2D(pool_size=2))\n",
    "model_AA_2.add(Dropout(0.2))\n",
    "\n",
    "model_AA_2.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model_AA_2.add(MaxPooling2D(pool_size=2))\n",
    "model_AA_2.add(Dropout(0.2))\n",
    "\n",
    "model_AA_2.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_AA_2.add(MaxPooling2D(pool_size=2))\n",
    "model_AA_2.add(Dropout(0.2))\n",
    "\n",
    "model_AA_2.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model_AA_2.add(MaxPooling2D(pool_size=2))\n",
    "model_AA_2.add(Dropout(0.2))\n",
    "model_AA_2.add(GlobalAveragePooling2D())\n",
    "\n",
    "model_AA_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_AA_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', # Loss\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "# Display model architecture summary\n",
    "model_AA_2.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 20\n",
    "num_batch_size = 40\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/simple/model_AA_2.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model_AA_2.fit(train_it,steps_per_epoch=1,epochs = 10, validation_data=val_it,validation_steps= 40,callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "loss = model_AA_2.evaluate(test_it)\n",
    "\n",
    "model_AA_2.save('./data/model_AA_new.keras')\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# import tensorflow as tf\n",
    "# model = tf.keras.models.load_model('./data/simple/model1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "noisy_model = tf.keras.models.load_model('models/model_noisy.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "#     img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = 'data/train/test_noisy/3220354.png'\n",
    "# img_path = 'data/train/test_accepted/3220357.png'\n",
    "img_path = 'data/train/test_accepted/3220683.png'\n",
    "# img_path = 'data/train/test_noisy/3220397.png'\n",
    "# img_path = 'data/train/test_noisy/3220403.png'\n",
    "# img_path = 'data/train/test_noisy/3220425.png'\n",
    "# img_path = 'data/train/test_noisy/3220685.png'\n",
    "# img_path = 'data/train/test_noisy/3220767.png'\n",
    "new_image = load_image(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "\n",
    "# test_image = image.load_img(img_path, target_size = (64, 64)) \n",
    "# test_image = image.img_to_array(test_image)\n",
    "# test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# datagen = ImageDataGenerator()\n",
    "# test_it = datagen.flow_from_directory('./data/train/',target_size=(224, 224), class_mode='binary',batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Done\n",
      "[[0.02154392]]\n",
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "print(\"Start\")\n",
    "# Y_pred = model_AA_2.predict(new_image, verbose = 1, batch_size = 40)\n",
    "Y_pred = noisy_model.predict(new_image, verbose = 1, batch_size = 40)\n",
    "print(\"Done\")\n",
    "print(Y_pred)\n",
    "y_pred = np.where(Y_pred > 0.5, 1,0)\n",
    "print(y_pred)\n",
    "# print(confusion_matrix(test_it.classes, y_pred))\n",
    "# print(accuracy_score(test_it.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accepted': 0, 'test_noisy': 1}\n"
     ]
    }
   ],
   "source": [
    "print(test_it.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2257 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_it = datagen.flow_from_directory('./data/simple/validation/',target_size=(224, 224), class_mode='binary',batch_size=40,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 22s 378ms/step\n",
      "[[ 956   44]\n",
      " [  55 1202]]\n",
      "0.9561364643331857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "Y_pred = model_AA_2.predict(val_it,verbose = 1,batch_size = 40)\n",
    "y_pred = np.where(Y_pred > 0.5, 1,0)\n",
    "print(confusion_matrix(val_it.classes, y_pred))\n",
    "print(accuracy_score(val_it.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_it.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Data Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1846 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "prod_Data = datagen.flow_from_directory('./data/prod_data/',target_size=(224, 224), class_mode='binary',batch_size=40,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 17s 361ms/step\n",
      "[[605 241]\n",
      " [ 19 981]]\n",
      "0.8591549295774648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "Y_pred = model.predict(prod_Data,verbose = 1,batch_size = 40)\n",
    "y_pred = np.where(Y_pred > 0.5, 1,0)\n",
    "print(confusion_matrix(prod_Data.2, y_pred))\n",
    "print(accuracy_score(prod_Data.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dataset Split Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2629/8000 [00:00<00:00, 26210.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 25879.30it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 56456.25it/s]\n",
      "100%|██████████| 676/676 [00:00<00:00, 35397.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# import os \n",
    "# from tqdm import tqdm\n",
    "\n",
    "# rdir = \"./data/noisy1/simple\"\n",
    "\n",
    "# list = os.listdir(rdir) # dir is your directory path\n",
    "# number_files = len(list)\n",
    "# print(number_files)\n",
    "\n",
    "# train = list[:8000]\n",
    "# val = list[8000:9000]\n",
    "# test = list[9000:]\n",
    "\n",
    "# for file in tqdm(train):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/train/noisy\"+\"/noisy_\"+file)\n",
    "\n",
    "# for file in tqdm(val):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/validation/noisy\"+\"/noisy_\"+file)\n",
    "\n",
    "# for file in tqdm(test):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/test/noisy\"+\"/noisy_\"+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Split Accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2648/8000 [00:00<00:00, 26477.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 39546.43it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 44180.33it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 59037.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# rdir = \"./data/accepted/simple\"\n",
    "\n",
    "# list = os.listdir(rdir) # dir is your directory path\n",
    "# number_files = len(list)\n",
    "# print(number_files)\n",
    "\n",
    "# train = list[:8000]\n",
    "# val = list[8000:9000]\n",
    "# test = list[9000:]\n",
    "\n",
    "# for file in tqdm(train):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/train/accepted\"+\"/accepted_\"+file)\n",
    "\n",
    "# for file in tqdm(val):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/validation/accepted\"+\"/accepted_\"+file)\n",
    "\n",
    "# for file in tqdm(test):\n",
    "#     os.rename(rdir+\"/\"+file,\"./data/simple/test/accepted\"+\"/accepted_\"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
